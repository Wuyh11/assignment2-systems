from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List, Optional

import torch
import torch.distributed as dist
import torch.nn as nn


@dataclass
class MinimalDDPFlat:
    """
    Minimal DDP wrapper:
      - broadcast parameters once at construction (rank 0 -> others)
      - after loss.backward(), call .sync_gradients() to average grads
        using ONE flattened all-reduce across all parameters
    """
    module: nn.Module
    process_group: Optional[dist.ProcessGroup] = None

    def __post_init__(self):
        if not dist.is_initialized():
            raise RuntimeError("torch.distributed is not initialized")

        self.rank = dist.get_rank(group=self.process_group)
        self.world_size = dist.get_world_size(group=self.process_group)

        # Ensure all ranks start from identical parameters: broadcast from rank 0
        for p in self.module.parameters():
            dist.broadcast(p.data, src=0, group=self.process_group)

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)

    @torch.no_grad()
    def sync_gradients(self):
        """
        Average gradients across ranks using a SINGLE all-reduce over a flattened buffer.
        Call this AFTER loss.backward() and BEFORE optimizer.step().
        """
        grads: List[torch.Tensor] = []
        params: List[torch.Tensor] = []

        # Collect grads (skip params with no grad, e.g. unused)
        for p in self.module.parameters():
            if p.grad is None:
                continue
            # DDP typically assumes dense grads; this assignment likely does too.
            if p.grad.is_sparse:
                raise NotImplementedError("Sparse gradients not supported in MinimalDDPFlat.")
            grads.append(p.grad.detach())
            params.append(p)

        if not grads:
            return  # nothing to do

        # Flatten into a single contiguous buffer
        flat = torch._utils._flatten_dense_tensors(grads)

        # All-reduce SUM across ranks
        dist.all_reduce(flat, op=dist.ReduceOp.SUM, group=self.process_group)

        # Average
        flat.div_(self.world_size)

        # Unflatten and copy back into original .grad tensors
        unflat = torch._utils._unflatten_dense_tensors(flat, grads)
        for g_dst, g_src in zip(grads, unflat):
            g_dst.copy_(g_src)

    def state_dict(self, *args, **kwargs):
        return self.module.state_dict(*args, **kwargs)

    def parameters(self, *args, **kwargs):
        return self.module.parameters(*args, **kwargs)
